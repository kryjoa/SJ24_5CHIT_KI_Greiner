{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd6914f7",
   "metadata": {},
   "source": [
    "# RAG-Pipeline (Retrieval-Augmented Generation) mit ChromaDB und FLAN-T5\n",
    "\n",
    "Large Language Models (LLMs) haben oft das Problem, dass ihr Wissen auf den Trainingszeitpunkt beschränkt ist und sie keine aktuellen oder domänenspezifischen Informationen abrufen können. Eine Retrieval-Augmented Generation (RAG)-Pipeline löst dieses Problem, indem sie externe Wissensquellen nutzt: Statt sich ausschließlich auf das Modellgedächtnis zu verlassen, durchsucht RAG eine Datenbank nach relevanten Informationen und kombiniert diese mit der generierten Antwort.\n",
    "\n",
    "In dieser Übung setzen wir eine RAG-Pipeline mit ChromaDB und FLAN-T5 um. ChromaDB dient als Vektordatenbank, in der wir Dokumente ablegen und über Embeddings nach relevanten Inhalten suchen können. FLAN-T5 wird als Sprachmodell genutzt, um eine Antwort auf Basis des abgerufenen Kontexts zu generieren. Diese Kombination ermöglicht es, präzisere, kontextbezogene Antworten zu erhalten – ein Ansatz, der insbesondere für Unternehmenswissen, technische Dokumentation oder domänenspezifische Fragen wertvoll ist.\n",
    "\n",
    "Quelle Abbildung: https://medium.com/@bijit211987/advanced-rag-for-llms-slms-5bcc6fbba411\n",
    "\n",
    "<img src=\"https://miro.medium.com/v2/resize:fit:4800/format:webp/1*q1CkGPwS7g4-f9rNbPrkig.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "85f2d0d0-fffd-4657-abbf-e09c93671f09",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!pip install torch transformers langchain langchain-community chromadb sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3f7837cf-4a7f-4bf7-a548-30dceca665d8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "from huggingface_hub import notebook_login\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7078ddec",
   "metadata": {},
   "source": [
    "Die Funktion `notebook_login()` wird verwendet, um sich in einem Hugging Face-Konto (https://huggingface.co) innerhalb eines Jupyter Notebook anzumelden. Dies ist erforderlich, wenn man auf private oder *gated* Modelle, Datasets oder Tokenizer von Hugging Face zugreifen möchte.\n",
    "\n",
    ">**Wichtig**: Dafür wird ein Hugging Face-Account benötigt. Zugangstokens können im Profil unter 'Access Tokens' erstellt werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d97ab06f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86e120a4a3e14328b10633566afec231",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8122fedb",
   "metadata": {},
   "source": [
    "Dieser Code erstellt eine Vektordatenbank mit ChromaDB, die semantische Suchen ermöglicht. Dadurch können später inhaltlich ähnliche Texte effizient gefunden werden.\n",
    "\n",
    "`HuggingFaceEmbeddings` wandelt Text in Vektoren um, um diese in der Vektor-DB speichern zu können. Die DB ist im Verzeichnis `./chroma_db` zu finden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2e69d02e-281f-43b5-847d-9fbae24a15ad",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cfd6c9d6-ea73-48c4-b851-803c63d67c4d',\n",
       " 'c2712105-27e5-4492-8fd2-1d676994e744',\n",
       " '6f091b86-44eb-4cf8-b4f2-8c168512aabc']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#huggingface_token = \"xxx\"\n",
    "\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "db = Chroma(persist_directory=\"./chroma_db\", embedding_function=embedding_model)\n",
    "\n",
    "docs = [\n",
    "    \"Apple hat 2024 einen Umsatz von 400 Milliarden USD gemacht.\",\n",
    "    \"Microsoft investiert massiv in Künstliche Intelligenz.\",\n",
    "    \"Tesla entwickelt neue Batterietechnologien für Elektroautos.\"\n",
    "]\n",
    "db.add_texts(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fdb22f7",
   "metadata": {},
   "source": [
    "`db.similarity_search(question, k=1)` sucht in der Vektordatenbank nach dem am besten passenden (semantisch ähnlichsten) Dokument. `k=1` bedeutet, dass nur das relevanteste Dokument zurückgegeben wird."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "67752f36-4c25-42f0-85e1-24fab4ffecf0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gefundener Kontext: Apple hat 2024 einen Umsatz von 400 Milliarden USD gemacht.\n"
     ]
    }
   ],
   "source": [
    "question = \"Wie viel Umsatz hat Apple 2024 gemacht?\"\n",
    "#question = \"Investiert Microsoft?\"\n",
    "\n",
    "retrieved_docs = db.similarity_search(question, k=1)\n",
    "\n",
    "context = retrieved_docs[0].page_content\n",
    "print(\"Gefundener Kontext:\", context)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e5f03b6",
   "metadata": {},
   "source": [
    "Hier wird das *FLAN-T5-Base-Modell* (Modell-ID: `google/flan-t5-base`) geladen. Muss nur 1x ausgeführt werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1671c75a-3577-4fc4-803f-54ed06bd873e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_name = \"google/flan-t5-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b04d3cf1",
   "metadata": {},
   "source": [
    "Abschließend muss der Eingabe-Prompt (inklusive Kontext) in Tokens umgewandelt werden, damit der Text von einem LLM verarbeitet werden kann."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bc6b02ef-5f89-45f6-a0c3-eb585894820d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Antwort: 400 billion USD\n"
     ]
    }
   ],
   "source": [
    "#prompt = f\"\\n\\nQuestion: {question}\\nAnswer:\"\n",
    "prompt = f\"Context: {context}\\n\\nQuestion: {question}\\nAnswer:\"\n",
    "\n",
    "# Tokenisierung\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = model.generate(**inputs, max_new_tokens=200)\n",
    "\n",
    "response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"Antwort:\", response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
